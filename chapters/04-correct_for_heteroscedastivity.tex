\section{Consequences of Heteroskedasticity}\label{sec:consequence_of_hetero}
In this section we wish to determine how \hetero will affect OLS. 
First we wish to determine if the estimator $\betahat$ is unbiased in the presence of heteroskedasticity.  

Remember assumption \ref{as:linear_in_the_parameters}, \ref{as:no_perfect_collinearity} and \ref{as:zero_conditional_mean}. If these hold, theorem \ref{th:unbiasedness_of_ols} proved that the OLS estimator $\betahat$ is unbiased for $\boldsymbol{\beta}$, which is the same as $E[\hat{\beta}_j] = \beta_j
$ for $j = 1,2, \ldots, k$. 
Because $\var(\boldsymbol{\varepsilon} | \mathbf{X})$ did not determine whether the OLS was biased or unbiased in theorem \ref{th:unbiasedness_of_ols} and its proof, the presence of \hetero will not cause $\betahat$ to be biased. 

Next we wish to determine how our measure-of-fit test $R$-squared is influenced by heteroskedasticity.

$R$-squared measures how much of the variance in the dependent variable is explained by the independent variables. In our model it determines how much of the variance in $y$ is explained by the independent variables $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_k$. 
Low values of $R$-squared makes predictions difficult because most of the variance in $y$ is caused by unobserved factors in $\boldsymbol{\varepsilon}$.
Consider the expression for $R$-squared
\begin{align*}
    R^2 = 1 - \dfrac{SSR}{SST}
\end{align*}
where $SSR$ is the sum of squared residuals given as
\begin{align*}
    SSR \equiv \nsum \hat{\varepsilon}_i^2
\end{align*}
and $SST$ is the total sum of squares given as
\begin{align*}
    SST \equiv \nsum (y_i - \overline{y})^2. 
\end{align*}
The $R$-squared value is therefore between $0$ and $1$. The closer the value is to one $1$ the more of the variance is accounted for in the explanatory variables, thus it is desirable to have an $R$-squared value close to $1$.
This coefficient of determination can be changed to the adjusted $R$-squared as
\begin{align}\label{eq:r_adjusted}
    R^2_{Adj} &= 1 - \dfrac{SSR/(n - k - 1)}{SST/(n - 1)}\\
      &= 1 - \dfrac{s^2}{SST/(n-1)}.
\end{align}
This rewriting of the coefficient of determination adjusts for the number of predictors in the model. This adjusted $R$-squared increases if a predictor improves more than the penalty of removing one degree of freedom. 

%Both $SSR$ and $SST$ is the unconditional variance in $R$-squared, thus they are not influenced by the presence of heteroskedasticity. 
%This can be seen as neither $SSR$ or $SST$ are dependent on the explanatory variables $\textbf{X}$.

The numerator in \eqref{eq:r_adjusted} is the estimate of the unconditional population variance of $\boldsymbol{\varepsilon}$ and the denominator is the estimate of the unconditional population variance of $\textbf{y}$. 
For example assume that $\var(\varepsilon_i | x_1, \ldots, x_k) = k \cdot x_i$. Then the estimate of the unconditional population variance of $\varepsilon_i$ is 
\begin{align}
     \var(\varepsilon_i) &= E[\var(\varepsilon_i | x_1, \ldots, x_k)] + \var(E[\varepsilon_i | x_1, \ldots, x_k]) \label{eq:lowoftotalvariance}\\
    &= E[k \cdot x_i] + \var(0) \nonumber \\
    &= k \cdot E[x_i] \nonumber
\end{align}
where \eqref{eq:lowoftotalvariance} is from the law of total variance. 
Because the last equality is a constant, heteroskedasticity will not cause the estimate of the unconditional population variance to be invalid, and thus R-squared is unaffected by heteroskedasticty \cite{Esben2019}.

This means that heteroskedasticity does not cause the OLS estimator to be biased and nor does it affect the $R$-squared coefficient.
It will however cause the variance of the OLS estimator, i.e. $\var(\betahat)$, to be biased, which will be explained next.  

The \homo assumption is $\var(\boldsymbol{\varepsilon} | \mathbf{X}) = \sigma^2$. 
If assumption \ref{as:linear_in_the_parameters}, \ref{as:no_perfect_collinearity}, \ref{as:zero_conditional_mean} and \ref{as:homoskedasticity_and_no_serial_correlation} hold, then the variance of the OLS estimator can be expressed as in theorem \ref{th:variance-covariance_of_the_ols_estimator}, which implies that the variance of $\betahat$ given the explanatory variables has an explicit form given as
\begin{align*}
    \var(\betahat | \mathbf{X}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}.
\end{align*}
The variance of the slope coefficients can be defined as
\begin{align}\label{eq:slope_variance_OLS_estimator}
    \var(\hat{\beta}_j|\mathbf{X}) = d_j = \dfrac{\sigma^2}{SST_j(1- R_j^2)},
\end{align}
where $j = 1, \ldots, k$ and $SST_j = \nsum (x_{ij} - \overline{x}_j)^2$ is the total sample variation in $x_j$ and $R^2_j$ is found from a regression on $x_j$ with all other independent variables. 

Equation \eqref{eq:slope_variance_OLS_estimator} depends on $\sigma^2$, $SST_j$ and $R^2_j$.
Here $\sigma^2$ is an unknown variable, the larger $\sigma^2$ the larger the variances for the OLS estimators. 
$SST_j$ is the total sample variation in $x_j$, the larger it is the smaller the variance of the OLS estimators so it is preferred to have as much sample variation as possible which can be obtained by increasing sample size. 
Notice that $R_j$-squared is found with a regression involving only the independent variables in the original model, where $x_j$ is the dependent variable. 
This differs from the $R$-squared found by regression on $y$ with $x_1, \ldots, x_k$ as the independent variables. 

Consider the example $y = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \varepsilon$, here $R_1$-squared is found by making a regression of $x_1$ on $x_2$. 
As always an $R_1$-squared value close to one means that $x_2$ explains much of the variation in $x_1$, so a value close to $1$ means that $x_1$ and $x_2$ are highly correlated.
$R_j$-squared tests how much of the variation in one of the independent variables is explained from the remaining independent variables.
In the case of multiple regression with more than 2 independent variables, this is known as multicollinearity.
The best estimator for $\hat{\beta}_j$ is found with a low value of $R_j$-squared, thus a smaller relationship between the explanatory variables, less multicollinearity, is desired when retrieving $\var(\hat{\beta}_j)$.
Note $R$-squared cannot equal $0$ due to assumption \ref{as:no_perfect_collinearity}. 

From \eqref{eq:slope_variance_OLS_estimator} it is possible to find the standard deviation of $\hat{\beta}_j$, which is the square root of \ref{eq:slope_variance_OLS_estimator}, which is
\begin{align}\label{eq:standard_deviation}
    sd(\hat{\beta}_j) = \dfrac{\sigma}{\left(SST_j(1- R_j^2)\right)^{1/2}}
\end{align}
The standard deviation, $\sigma$, is replaced by its estimate to obtain the standard error
\begin{align}\label{eq:standad_error}
    se(\hat{\beta}_j) = \dfrac{s}{\big(SST_j(1- R_j^2)\big)^{1/2}}
\end{align}
Note that the standard deviation measures the dispersion a dataset has from the mean, whereas the standard errors measures how far the sample mean of the data is likely to be from the true sample space mean. 

Since \eqref{eq:standad_error} is obtained from \eqref{eq:slope_variance_OLS_estimator}, which relies on \homo the standard error will not be a feasible estimator of the standard deviation in \eqref{eq:standard_deviation} in the presence of heteroskedasticity. 
This means that \hetero will cause bias in $\var(\hat{\beta}_j)$ which invalidates the assumption of the error term $\varepsilon$.
Therefore if the assumption of homoskedasticity is violated, the variance of the slope coefficients will be biased. 
Because the $t$-test and $F$-test are based upon $\var(\hat{\beta}_j)$ they will no longer be valid. 
% the error terms will not be normally distributed and thus
As a result \textit{confidence intervals}, \textit{t-statistics} and \textit{F-statistics} will be invalid when \hetero is present. 
These statistics will be introduced next.

During this chapter we have found that under the Gauss-Markov assumptions;
\begin{enumerate}
    \item Linear in parameters
    \item No perfect collinearity
    \item Zero conditional mean
    \item Homoskedasticity and no serial correlation
\end{enumerate}
 the OLS estimator is the BLUE. 
 In addition we have found an unbiased estimator for the variance of the error term conditional on $\textbf{X}$.