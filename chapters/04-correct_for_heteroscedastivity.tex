\newpage
\section{Consequences of Heteroscedasticity}
In this sections we wish to determine how \hetero will affect both OLS, \textit{t-statistics}, \textit{F-statistics} and \textit{confidence intervals}. 

First we wish to determine if the estimator $\betahat$ is unbiased or biased in the presence of heteroskedasticity.  

Remember assumption \ref{as:linear_in_the_parameters}, \ref{as:no_perfect_collinearity} and \ref{as:zero_conditional_mean}. If these hold theorem \ref{th:unbiasedness_of_ols} proved that the OLS estimator $\betahat$ is unbiased for $\boldsymbol{\beta}$, which is the same as $E[\hat{\beta}_j] = \beta_j
$ for $j = 1,2, \ldots, k$. 

Because the error terms in $\var(\boldsymbol{\varepsilon} | \mathbf{X})$ did no determine whether the OLS was biased or unbiased in the theorem and its proof, the presence of \hetero will not cause $\betahat$ to be biased. 

Next we wish to determine how our measure-of-fit test $\mathcal{R}^2$ is influenced by \hetero. 
$\mathcal{R}^2$ measures how much of the variance in the dependent variable is explained by the independent variables. In our model it determines how much of the variance in $y$ is explained in the variables $\mathbf{X}$. 
Low values of $\mathcal{R}^2$ makes predictions difficult because most of the variance in $y$ is caused by unobserved factors in $\varepsilon$, so the OLS predictions will be hard to calculate given a set of explanatory variables.
Consider the expression for $\mathcal{R}^2$
\begin{align*}
    \mathcal{R}^2 = 1 - \dfrac{SSR}{SST}
\end{align*}
Where \textbf{SSR} is the \textbf{sum of squared residuals} given as
\begin{align*}
    SSR \equiv \nsum \hat{\varepsilon}_i^2
\end{align*}
and \textbf{SST} is the \textbf{total sum of squares} given as
\begin{align*}
    SST \equiv \nsum (y_i - \overline{y})^2. 
\end{align*}
The $\mathcal{R}^2$ value is between $0$ and $1$. The closer the value is to one $1$ the more of the variance is accounted for in the explanatory variables, thus it is desireable to have an $\mathcal{R}^2$ value close to $1$.
This coefficient of determination can be changed to the adjusted $\mathcal{R}^2$ as
\begin{align*}
    \mathcal{R}^2_{Adj} =& 1 - \dfrac{SSR/(n - k - 1)}{SST/(n - 1)}\\
      =& 1 - \dfrac{\hat{\sigma}^2}{SST/(n-1)}
\end{align*}
This rewriting of the model adjusts for the number of predictors in the model. This $\mathcal{R}^2$ increases if a predictor improves more than penalty of removing one degree of freedom. 

Both SSR and SST are unconditional variances in $\mathcal{R}^2$, thus they are not influenced by the presence of heteroskedasticity. This can be seen as neither SSR or SST are dependant on the explanatory variables $\textbf{X}$. 

This means that heteroskedasticity does not cause the OLS estimators to be biased or inconsistent and nor does it affect the $\mathcal{R}^2$ test. 
It will however cause the variance of the OLS estimators, i.e. $\var(\betahat)$, to be biased, which will be explained next.  

The \homo \ assumption is $\var(\boldsymbol{\varepsilon} | \mathbf{X}) = \sigma^2$. If assumption \ref{as:linear_in_the_parameters}, \ref{as:no_perfect_collinearity}, \ref{as:zero_conditional_mean} and \ref{as:homoskedasticity_and_no_serial_correlation} hold the variance of the OLS estimator can be expressed as in theorem \ref{th:variance-covariance_of_the_ols_estimator}, which implies that variance of $\betahat$ given the explanatory variables has an explicit form given as
\begin{align*}
    \var(\betahat | \mathbf{X}) = \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}.
\end{align*}

\subsection{Residuals}\label{subsec:residuals}

The residuals are the difference between the observed values $y$ and the fitted values $\hat{y}$. They are therefore the observed errors $\boldsymbol{\hat{\varepsilon}}$ described in \eqref{eq:replace_with_y}
\begin{align*}
    r(\textbf{y}) &= \textbf{y} - \hat{\textbf{y}} = \textbf{y} - \textbf{X} \betahat = (I - H) \textbf{y}.
    % r_i(y) &= y_i - x_i \betahat = y_i - x_i (X^\top X)^{-1}X^\top y_i = (1 - h_{ii}) y_i,
\end{align*}
By the assumption $\varepsilon \sim N(0, \sigma^2)$ the residuals have variance
\begin{align*}
    Var(r(\textbf{Y})) &= \sigma^2 (I - H) \\
    Var(r_i(\textbf{Y})) &= \sigma^2(1 - h_{ii}),
\end{align*}
where $h_{ii}$ is the i'th diagonal element in $H$. The second equation follows, since the first equation is just the variance-covariance matrix, which has variances in the diagonal.
Rewriting \eqref{eq:OLS_estimate_sigma_i_anden} using projection matrices gives the following estimate for the parameter variance
\begin{align*}
    \hat{\sigma}^2(\textbf{Y}) &= \frac{\| Y - X \betahat (Y) \|^2 }{n-k-1} = \frac{\| y - H y \|^2}{n-k-1} 
\end{align*}
We now introduce the notation $-i$, which refers to the removal of the i'th row e.g.
\begin{align*}
    \textbf{y}_{-i}^\top = (y_1, \ldots, y_{i-1}, y_{i+1}, \ldots, y_n)^\top
\end{align*}
The estimate for $\sigma^2$ found by deleting the i'th row is then
\begin{align*}
    \hat{\sigma}^2_i(\textbf{Y}) = \frac{\| y_{-i} - H_{-i} y_{-i} \|^2}{n-k-1}.
\end{align*}
Residuals can be used to identfiy outliers, that is observation that do not fit the general pattern of the data. These datapoints can have a large effect on the estimated variance and it would therefore be useful to have a way to measure of how much variance an observation adds. We now introduce the studentized residual
\begin{align*}
    r_i^{rt} = \frac{r_i}{\sqrt{\hat{\sigma^2_{(i)}}(1-h_{11})}}
\end{align*}
If this value in deemed to high, it can justify removing it as an outlier/measurement error. 

\begin{proposition}
    Consider a general linear model $Y \sim N_n(X \betahat, \sigma^2 I_n)$ where the design matrix has full rank $k \leq n-2$ and $H = X (X^\top X)^{-1}X^\top$ is the corresponding projection-matrix, then
    \begin{align*}
        r_i^{rt}(\textbf{Y}) \sim t(n-k-1).
    \end{align*}
\end{proposition}
\begin{proof}
    Without loss of generality let $i=1$. Then 
\end{proof}

The slope coefficients can be defined as 
\begin{align}\label{eq:slope_variance_OLS_estimator}
    \var(\hat{\beta}_j|\mathbf{X}) = \dfrac{\sigma^2}{SST_j(1- \mathcal{R}_j^2)}
\end{align}
Where $j = 1, \ldots, k$ and $SST_j = \nsum (x_{ij} - \overline{x}_j)^2$ is the total sample variation in $x_j$ and $\mathcal{R}^2_j$ is found from a regression on $x_j$ with all other independent variables. 

Equation \eqref{eq:slope_variance_OLS_estimator} depends on $\sigma^2$, $SST_j$ and $\mathcal{R}^2_j$.
Here $\sigma^2$ is an unknown variable, the larger $\sigma^2$ the larger are the variances for the OLS estimators. $SST_j$ is the total sample variation in $x_j$, the larger it is the smaller will the variance of the OLS estimators be so it is preferred to have as much sample variation as possible which can be obtained by increasing sample size. 
Notice that $\mathcal{R}^2_j$ is found with a regression involving only the independent variables in the original model, where $x_j$ is the dependent variable. 
This differs from the $\mathcal{R}^2$ found by regression on $y$ with $x_1, \ldots, x_k$ as the independent variables. 

Consider the example $y = \betahat_0 + \betahat_1 x_1 + \betahat_2 x_2 + \varepsilon$, here $\mathcal{R}^2_1$ is found by making a regression of $x_1$ on $x_2$. 
As always an $\mathcal{R}^2_1$ value close to one means that $x_2$ explains much of the variation in $x_1$, so a value close to $1$ means that $x_1$ and $x_2$ are highly correlated.
$\mathcal{R}^2_j$ tests how much of the variation in one of the independent variables is explained from the remaining independent variables.
The best estimator for $\betahat_j$ is found with a low value of $\mathcal{R}^2_j$, thus a smaller relationship between the explanatory variables is desired when retrieving $\var(\betahat_j)$. Note $\mathcal{R}^2$ cannot equal $0$ due to assumption \ref{as:no_perfect_collinearity}. 

From \eqref{eq:slope_variance_OLS_estimator} it is possible to find the standard deviation of $\betahat_j$, which is the squre root of \ref{eq:slope_variance_OLS_estimator}, which is
\begin{align}\label{eq:standard_deviation}
    sd(\betahat_j) = \dfrac{\sigma}{SST_j(\big(1- \mathcal{R}_j^2)\big)^{1/2}}
\end{align}
The standard deviation, $\sigma$, is replaced by its estimate to obtain the standard error
\begin{align}\label{eq:standad_error}
    se(\betahat_j) = \dfrac{\hat{\sigma}}{SST_j(\big(1- \mathcal{R}_j^2)\big)^{1/2}}
\end{align}
Note that the standard deviation measures the dispersion a data set has from the mean, whereas the standard errors measures how far the sample mean of the data is likely to be from the true sample space mean. 

Since \eqref{eq:standad_error} is obtained from \eqref{eq:slope_variance_OLS_estimator}, which relies on \homo the standard error will not be a valid estimator of the standard deviation in \eqref{eq:standard_deviation} in the presence of \hetero. This means that \hetero will cause bias in the $\var(\betahat_j)$ which invalidates the standard errors, $\varepsilon$. 

Furthermore \textit{confidence intervals}, \textit{t-statistics} and \textit{F-statistics} will no longer follow a normal distribution and thus their results will be invalid when \hetero is present. 

First it is needed to estimate the estimators of variance, $\var(\betahat)$, when \hetero is present. 

\subsection{Correct for Heteroscedasticity}
If we sum up \hetero will in general cause OLS standard errors to be faulty as well as the related statistical tests. In this section we wish to correct for these mistakes, so they are correct when \hetero is present.
\subsection{Test for Heteroscedasticity}

**Afsnit 8.4 siger at man kan få en bedre estimator end OLS når man kender formen på hetero**

In this section we wish to determine how to detect heteroskedasticity and the type of heteroskedasticity. 

We consider our linear model given as $y = \mathbf{\beta}_0 + \mathbf{\beta}_1x_1 + \ldots + \mathbf{\beta}_kx_k + \mathbf{\varepsilon}$ and let assumption \ref{as:linear_in_the_parameters}, \ref{as:no_perfect_collinearity}, \ref{as:zero_conditional_mean} hold true. Especially that the OLS is unbiased and consistent because $E[\varepsilon | \mathbf{X}] = 0$. 

We then construct out null hypothesis as
\begin{align*}
    \mathcal{H}_0 : \var[\varepsilon | \mathbf{X}] = \var[\varepsilon] = \sigma^2. 
\end{align*}
With this null hypothesis we wish to determine if the homoskedasticity assumption holds. If we cannot reject our $\mathcal{H}_0$-hypothesis for small significance value then heteroskedasticity is not present. Because it is not possible to accept a $\mathcal{H}_0$-hypothesis, but only reject or fail to reject the hypothesis we instead recall assumption \ref{as:zero_conditional_mean}, which tells the conditional expected value is $0$. This assumption implies that $\var(\varepsilon | \mathbf{X}) = E[\varepsilon^2 | \mathbf{X}]$. We can thus rewrite the $\mathcal{H}_0$-hypothesis as
\begin{align*}
    \mathcal{H}_0 : E[\varepsilon^2 | \mathbf{X}] = E[\varepsilon^2] = \sigma^2.
\end{align*}
With this $\mathcal{H}_0$-hypothesis we test if the error term, $\sigma^2$ is related with any of the explanatory variables. 
If $\mathcal{H}_0$ is rejected then $\varepsilon^2$ is a function of a at least one the explanatory variables, so the error term can be expressed as
\begin{align}\label{eq:test_hetero_nul_hypotese}
    \varepsilon^2 = \delta_0 + \delta_1x_1 + \ldots + \delta_kx_k + v
\end{align}
Where $v$ is an error with mean equal to $0$. For this to be homoskedastic the following $\mathcal{H}_0$-hypothesis must fail to be rejected
\begin{align}\label{eq:H_nul_for_hetero_med_delta}
    \mathcal{H}_0 : \delta_1 = \delta_2 = \ldots = \delta_k = 0
\end{align}
If the error $v$ in \eqref{eq:test_hetero_nul_hypotese} is assumed to be independent of the explanatory variables $\mathbf{X}$, then F-statistics can be used to test \eqref{eq:H_nul_for_hetero_med_delta} for the overall significance of the explanatory variables for $\varepsilon^2$.
Because $\varepsilon^2$ is unobserved we cannot make an OLS regression on $\varepsilon^2$ for the explanatory variables, but we can make the regression for its estimate as
\begin{align*}
    \hat{\varepsilon}^2 = \delta_0 + \delta_1x_1 + \ldots + \delta_kx_k + error
\end{align*}





